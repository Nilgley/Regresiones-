{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis de Regresión (II)\n",
    "\n",
    "En este cuaderno, veremos cómo el análisis de regresión puede ayudar a **entender el comportamiento de los datos**, **predecir valores de datos** (continuos o dicotómicos), y **encontrar predictores importantes** (modelos dispersos).\n",
    "Presentamos diferentes modelos de regresión: regresión lineal simple, regresión lineal múltiple y regresión polinómica.\n",
    "Evaluamos los resultados cualitativamente mediante herramientas de visualización de Seaborn y cuantitativamente mediante la biblioteca Scikit-learn, así como otras herramientas.\n",
    "\n",
    "Usamos diferentes conjuntos de datos reales:\n",
    "* Conjunto de datos macroeconómicos\n",
    "* Predicción del precio de un nuevo mercado de viviendas\n",
    "* Extensión del hielo marino y cambio climático\n",
    "* Conjunto de datos de diabetes de Scikit-learn\n",
    "* Conjunto de datos Longley de datos macroeconómicos de EE. UU.\n",
    "* Conjunto de datos de publicidad\n",
    "\n",
    "### Contenidos del cuaderno:\n",
    "\n",
    "- Regresión Lineal Múltiple\n",
    "- Regularización: Ridge y Lasso\n",
    "- Transformación de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings for the visualizations\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline \n",
    "plt.rc('font', size=12) \n",
    "plt.rc('figure', figsize = (12, 5))\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"notebook\", font_scale=1, rc={\"lines.linewidth\": 2,'font.family': [u'times']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1 # to make this notebook's output stable across runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo 1: Vivienda en Boston\n",
    "\n",
    "Continuemos con nuestro conjunto de datos de Vivienda en Boston."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datasets\n\u001b[1;32m----> 3\u001b[0m boston \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mload_boston() \u001b[38;5;66;03m# Dictionary-like object that exposes its keys as attributes.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m X,y \u001b[38;5;241m=\u001b[39m boston\u001b[38;5;241m.\u001b[39mdata, boston\u001b[38;5;241m.\u001b[39mtarget \u001b[38;5;66;03m# Create X matrix and y vector from the dataset.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m features \u001b[38;5;241m=\u001b[39m boston\u001b[38;5;241m.\u001b[39mfeature_names\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\datasets\\__init__.py:157\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_boston\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    109\u001b[0m     msg \u001b[38;5;241m=\u001b[39m textwrap\u001b[38;5;241m.\u001b[39mdedent(\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;124m        `load_boston` has been removed from scikit-learn since version 1.2.\u001b[39m\n\u001b[0;32m    111\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;124m        <https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\u001b[39m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;124m        \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[1;32m--> 157\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m()[name]\n",
      "\u001b[1;31mImportError\u001b[0m: \n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "from sklearn import datasets\n",
    "boston = datasets.load_boston() # Dictionary-like object that exposes its keys as attributes.\n",
    "X,y = boston.data, boston.target # Create X matrix and y vector from the dataset.\n",
    "features = boston.feature_names\n",
    "print('feature names: {}'.format(boston.feature_names))\n",
    "print('Shape of data: {} {}'.format(X.shape, y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train test split\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m----> 3\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m, random_state\u001b[38;5;241m=\u001b[39mseed)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain and test sizes of X: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(X_train\u001b[38;5;241m.\u001b[39mshape, X_test\u001b[38;5;241m.\u001b[39mshape))\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain and test sizes of y: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(y_train\u001b[38;5;241m.\u001b[39mshape, y_test\u001b[38;5;241m.\u001b[39mshape))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "# Train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.9, random_state=seed)\n",
    "\n",
    "print('Train and test sizes of X: {} {}'.format(X_train.shape, X_test.shape))\n",
    "print('Train and test sizes of y: {} {}'.format(y_train.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Fitting a multiple linear model\u001b[39;00m\n\u001b[0;32m      2\u001b[0m lr \u001b[38;5;241m=\u001b[39m LinearRegression() \u001b[38;5;66;03m# Create the Linear Regression estimator\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m lr\u001b[38;5;241m.\u001b[39mfit(X_train, y_train) \u001b[38;5;66;03m# Perform the fitting\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Regrerssion coefs\u001b[39;00m\n\u001b[0;32m      7\u001b[0m coefs_lr \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(np\u001b[38;5;241m.\u001b[39mabs(lr\u001b[38;5;241m.\u001b[39mcoef_), features)\u001b[38;5;241m.\u001b[39msort_values()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Fitting a multiple linear model\n",
    "lr = LinearRegression() # Create the Linear Regression estimator\n",
    "lr.fit(X_train, y_train) # Perform the fitting\n",
    "\n",
    "\n",
    "# Regrerssion coefs\n",
    "coefs_lr = pd.Series(np.abs(lr.coef_), features).sort_values()\n",
    "\n",
    "# Prediction\n",
    "y_test_pred = lr.predict(X_test)\n",
    "\n",
    "# evaluation\n",
    "mse = mean_squared_error(y_test, y_test_pred)\n",
    "r2score_train = lr.score(X_train, y_train)\n",
    "r2score_test = lr.score(X_test, y_test)\n",
    "\n",
    "# The coefficients\n",
    "print('\\nIntercept and coefs:\\n{} {}'.format(lr.intercept_, lr.coef_))\n",
    "# The mean squared error\n",
    "print('\\nMSE: {}'.format(mse))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print('R^2 Score: {}'.format(r2score_train))\n",
    "print('R^2 Score: {}'.format(r2score_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'coefs_lr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Plotting abs value of model coefficients\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m coefs_lr\u001b[38;5;241m.\u001b[39mplot(kind\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbar\u001b[39m\u001b[38;5;124m'\u001b[39m, title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel Coefficients\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'coefs_lr' is not defined"
     ]
    }
   ],
   "source": [
    "# Plotting abs value of model coefficients\n",
    "coefs_lr.plot(kind='bar', title='Model Coefficients')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que todos los coeficientes obtenidos son diferentes de cero, lo que significa que no se descarta ninguna variable.\n",
    "A continuación, intentaremos construir un nuevo modelo para predecir el precio utilizando los factores más importantes y descartando los no informativos. Para hacer esto, podemos crear un regresor LASSO, forzando coeficientes a cero (ver más abajo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'boston' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(boston\u001b[38;5;241m.\u001b[39mDESCR)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'boston' is not defined"
     ]
    }
   ],
   "source": [
    "print(boston.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos de Regularización\n",
    "\n",
    "### Regularización L2: Regresión Ridge\n",
    "\n",
    "La Regresión Ridge penaliza los coeficientes si estos están demasiado alejados de cero, obligándolos a ser pequeños de manera continua. De esta manera, reduce la complejidad del modelo mientras mantiene todas las variables en el modelo.\n",
    "\n",
    "$$ minimize(\\sum_{i=0}^n (y_i - \\beta_0- \\sum_{j=1}^p \\beta_jx_{ij})^2 + \\alpha\\sum_{j=1}^p \\beta_j^2) $$\n",
    "\n",
    "donde $\\beta_j$ son los coeficientes de regresión.\n",
    "\n",
    "\n",
    "### Regularización L1: Regresión Lasso\n",
    "\n",
    "A menudo, en problemas reales, hay variables no informativas en los datos que impiden un modelado adecuado del problema y, por lo tanto, la construcción de un modelo de regresión correcto. En tales casos, un proceso de selección de características es crucial para seleccionar solo las características informativas y descartar las no informativas. Esto se puede lograr mediante métodos dispersos que utilizan un enfoque de penalización, como *Lasso* (operador de encogimiento y selección absoluta mínima) para establecer algunos coeficientes del modelo a cero (descartando así esas variables). La dispersión puede verse como una aplicación de la navaja de Occam: preferir modelos más simples a los complejos.\n",
    "\n",
    "Para ello, la regresión Lasso añade un término de regularización de **norma $\\ell_1$** a la suma de errores cuadráticos de predicción (SSE). Dado el conjunto de muestras (𝑋,𝐲), el objetivo es minimizar:\n",
    "\n",
    "$$ minimize(\\sum_{i=0}^n (y_i - \\beta_0- \\sum_{j=1}^p \\beta_jx_{ij})^2 + \\alpha\\sum_{j=1}^p|\\beta_j|)$$\n",
    "\n",
    "### Interpretación geométrica de la regularización\n",
    "\n",
    "El panel izquierdo muestra la regularización L1 (regularización lasso) y el panel derecho la regularización L2 (regresión Ridge). Las elipses indican la distribución para no regularización. Las formas (cuadrado y círculo) muestran las restricciones debido a la regularización (limitando $\\theta^2$ para la regresión Ridge y $|\\theta|$ para la regresión Lasso). Las esquinas de la regularización L1 crean más oportunidades para que la solución tenga ceros en algunos de los pesos.\n",
    "\n",
    "<center><img src=\"files/images/regularization-ridge-lasso.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Más información [aquí](https://www.datacamp.com/community/tutorials/tutorial-ridge-lasso-elastic-net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ridge Regression\n",
    "ridge = linear_model.Ridge(alpha=1) # Create a Ridge regressor\n",
    "ridge.fit(X_train, y_train) # Perform the fitting\n",
    "\n",
    "# Regrerssion coefs\n",
    "coefs_ridge = pd.Series(np.abs(ridge.coef_), features).sort_values()\n",
    "\n",
    "# Prediction\n",
    "y_test_pred_ridge = ridge.predict(X_test)\n",
    "\n",
    "# evaluation\n",
    "mse_ridge = mean_squared_error(y_test, y_test_pred_ridge)\n",
    "r2score_ridge_train = ridge.score(X_train, y_train)\n",
    "r2score_ridge_test = ridge.score(X_test, y_test)\n",
    "\n",
    "# The coefficients\n",
    "print('\\nIntercept and coefs:\\n{} {}'.format(ridge.intercept_, ridge.coef_))\n",
    "# The mean squared error\n",
    "print('\\nMSE: {}'.format(mse_ridge))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print('R^2 Score train: {}'.format(r2score_ridge_train))\n",
    "print('R^2 Score test: {}'.format(r2score_ridge_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting abs value of model coefficients\n",
    "coefs_ridge.plot(kind='bar', title='Ridge Coefficients')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Para completar:**\n",
    "Ajusta un regresor Lasso y evalúalo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lasso Regression\n",
    "lasso = linear_model.Lasso(alpha=1)\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "# Regrerssion coefs\n",
    "coefs_lasso = pd.Series(np.abs(lasso.coef_), features).sort_values()\n",
    "\n",
    "# Prediction\n",
    "y_test_pred_lasso = lasso.predict(X_test)\n",
    "\n",
    "# evaluation\n",
    "mse_lasso = mean_squared_error(y_test, y_test_pred_lasso)\n",
    "\n",
    "r2score_lasso_train = lasso.score(X_train, y_train)\n",
    "r2score_lasso_test = lasso.score(X_test, y_test)\n",
    "\n",
    "\n",
    "# The coefficients\n",
    "\n",
    "# The mean squared error\n",
    "\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print(mse_lasso)\n",
    "print(r2score_lasso_train, r2score_lasso_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting abs value of model coefficients\n",
    "coefs_lasso.plot(kind='bar', title='Lasso Coefficients')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs_lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparar los resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are the coeficients now sparse?\n",
    "# Is the score different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(15,5))\n",
    "ax1 = f.add_subplot(131)\n",
    "ax2 = f.add_subplot(132)\n",
    "ax3 = f.add_subplot(133)\n",
    "\n",
    "coefs_lr.plot(kind=\"barh\", title='coefs_lr', ax=ax1)\n",
    "coefs_ridge.plot(kind=\"barh\", title='coefs_ridge', ax=ax2)\n",
    "coefs_lasso.plot(kind=\"barh\", title='coefs_lasso', ax=ax3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Non important variables: {}'.format(coefs_lasso.index[coefs_lasso==0].values))\n",
    "print('Most important variable: {}'.format(coefs_lasso.index[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [[r2score_train, r2score_test],\n",
    "         [r2score_ridge_train, r2score_ridge_test],\n",
    "         [r2score_lasso_train, r2score_lasso_test]]\n",
    "df_scores = pd.DataFrame(scores, columns=[\"Train\", \"Test\"], index=[\"No regularization\", \"Ridge\", \"Lasso\"])\n",
    "#df_scores.sort_values(by=\"test_score\", ascending=False, inplace=True)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforma y Predice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at mean and average values of our predictors\n",
    "for i, feat in enumerate(features):\n",
    "    print()\n",
    "    print(feat)\n",
    "    print(\"Max {}, min {}, mean {}, and var {}\".format(np.max(X[:, i]), np.min(X[:, i]), np.mean(X[:, i]), np.var(X[:, i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existe un tipo especial de ``Estimator`` llamado ``Transformer`` que transforma los datos de entrada, por ejemplo, selecciona un subconjunto de las características o extrae nuevas características basadas en las originales.\n",
    "\n",
    "Un transformador que utilizaremos aquí es ``sklearn.preprocessing.StandardScaler``. Este transformador centra cada predictor en ``X`` para tener media cero y varianza unitaria y es útil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scalerX = StandardScaler().fit(X_train) # Create the transformer StandardScaler and perform the fitting for the training data\n",
    "\n",
    "X_train_norm = scalerX.transform(X_train)\n",
    "X_test_norm = scalerX.transform(X_test)\n",
    "\n",
    "print(\"\\nBefore transformation:\")\n",
    "print('Train: Max {}, min {}, mean {}, and var {}'.format(np.max(X_train), np.min(X_train), np.mean(X_train), np.var(X_train)))\n",
    "print('Test: Max {}, min {}, mean {}, and var {}'.format(np.max(X_test), np.min(X_test), np.mean(X_test), np.var(X_test)))\n",
    "\n",
    "print(\"\\nAfter transformation:\")\n",
    "print('Train: Max {}, min {}, mean {}, and var {}'.format(np.max(X_train_norm), np.min(X_train_norm), np.mean(X_train_norm), np.var(X_train_norm)))\n",
    "print('Test: Max {}, min {}, mean {}, and var {}'.format(np.max(X_test_norm), np.min(X_test_norm), np.mean(X_test_norm), np.var(X_test_norm)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, feat in enumerate(features):\n",
    "    print()\n",
    "    print(feat)\n",
    "    print(\"\\nBefore transformation:\")\n",
    "    print('Train: Max {}, min {}, mean {}, and var {}'.format(np.max(X_train[:, i]), np.min(X_train[:, i]), np.mean(X_train[:, i]), np.var(X_train[:, i])))\n",
    "    print('Test: Max {}, min {}, mean {}, and var {}'.format(np.max(X_test[:, i]), np.min(X_test[:, i]), np.mean(X_test[:, i]), np.var(X_test[:, i])))\n",
    "\n",
    "    print(\"\\nAfter transformation:\")\n",
    "    print('Train: Max {}, min {}, mean {}, and var {}'.format(np.max(X_train_norm[:, i]), np.min(X_train_norm[:, i]), np.mean(X_train_norm[:, i]), np.var(X_train_norm[:, i])))\n",
    "    print('Test: Max {}, min {}, mean {}, and var {}'.format(np.max(X_test_norm[:, i]), np.min(X_test_norm[:, i]), np.mean(X_test_norm[:, i]), np.var(X_test_norm[:, i])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora comparemos los coeficientes que obtendríamos si usáramos la matriz de variables estandarizadas en su lugar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "lr_norm = linear_model.LinearRegression()\n",
    "ridge_norm = linear_model.Ridge(alpha=.3)\n",
    "lasso_norm = linear_model.Lasso(alpha=.3)\n",
    "\n",
    "lr_norm.fit(X_train_norm, y_train)\n",
    "ridge_norm.fit(X_train_norm, y_train)\n",
    "lasso_norm.fit(X_train_norm, y_train)\n",
    "\n",
    "coefs_lr_norm = pd.Series(np.abs(lr_norm.coef_), boston.feature_names).sort_values()\n",
    "coefs_ridge_norm = pd.Series(np.abs(ridge_norm.coef_), boston.feature_names).sort_values()\n",
    "coefs_lasso_norm = pd.Series(np.abs(lasso_norm.coef_), boston.feature_names).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(15,5))\n",
    "ax1 = f.add_subplot(131)\n",
    "ax2 = f.add_subplot(132)\n",
    "ax3 = f.add_subplot(133)\n",
    "\n",
    "coefs_lr.plot(kind=\"barh\", title='coefs_lr', ax=ax1)\n",
    "coefs_ridge.plot(kind=\"barh\", title='coefs_ridge', ax=ax2)\n",
    "coefs_lasso.plot(kind=\"barh\", title='coefs_lasso', ax=ax3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(15,5))\n",
    "ax1 = f.add_subplot(131)\n",
    "ax2 = f.add_subplot(132)\n",
    "ax3 = f.add_subplot(133)\n",
    "\n",
    "coefs_lr_norm.plot(kind=\"barh\", title='coefs_lr_norm', ax=ax1)\n",
    "coefs_ridge_norm.plot(kind=\"barh\", title='coefs_ridge_norm', ax=ax2)\n",
    "coefs_lasso_norm.plot(kind=\"barh\", title='coefs_lasso_norm', ax=ax3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Non important variables:')\n",
    "print('Before transformation: {}'.format(sorted(coefs_lasso.index[coefs_lasso_norm==0].values)))\n",
    "print('After transformation: {}'.format(sorted(coefs_lasso_norm.index[coefs_lasso_norm==0].values)))\n",
    "print('Most important variable:')\n",
    "print('Before transformation: {}'.format(coefs_lasso.index[-1]))\n",
    "print('After transformation: {}'.format(coefs_lasso_norm.index[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scores\n",
    "print('lr: {}'.format(lr.score(X_test, y_test)))\n",
    "print('ridge: {}'.format(ridge.score(X_test, y_test)))\n",
    "print('lasso: {}'.format(lasso.score(X_test, y_test)))\n",
    "print('lr_norm: {}'.format(lr_norm.score(X_test_norm, y_test)))\n",
    "print('ridge_norm: {}'.format(ridge_norm.score(X_test_norm, y_test)))\n",
    "print('lasso_norm: {}'.format(lasso_norm.score(X_test_norm, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajustando los regresores Ridge y Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_alphas = 100\n",
    "alphas = np.logspace(-2, 2, n_alphas)\n",
    "\n",
    "coefs_ridge = []\n",
    "r2_ridge = []\n",
    "for l in alphas:\n",
    "    regr_ridge = linear_model.Ridge(alpha=l) # Create a Ridge regressor\n",
    "    regr_ridge.fit(X_train_norm, y_train)  # Perform the fitting\n",
    "    coefs_ridge.append(regr_ridge.coef_)\n",
    "    r2_ridge.append(regr_ridge.score(X_test_norm,y_test))\n",
    "\n",
    "    \n",
    "coefs_lasso = []\n",
    "r2_lasso = []\n",
    "for l in alphas:\n",
    "    regr_lasso = linear_model.Lasso(alpha=l,tol =0.001) # Create a Ridge regressor\n",
    "    regr_lasso.fit(X_train_norm, y_train)  # Perform the fitting\n",
    "    coefs_lasso.append(regr_lasso.coef_)\n",
    "    r2_lasso.append(regr_lasso.score(X_test_norm,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(20, 20), sharey='row')\n",
    "\n",
    "\n",
    "axs[0,0].plot(alphas, np.abs(coefs_ridge))\n",
    "axs[0,0].set_xscale('log')\n",
    "axs[0,0].set_title('Ridge coefficients as a function of the regularization')\n",
    "axs[0,0].axis('tight')\n",
    "axs[0,0].set_xlabel('alpha')\n",
    "axs[0,0].set_ylabel('weights')\n",
    "axs[0,0].legend(boston.feature_names)\n",
    "\n",
    "axs[0,1].plot(alphas, np.abs(coefs_lasso))\n",
    "axs[0,1].set_xscale('log')\n",
    "axs[0,1].set_title('Lasso coefficients as a function of the regularization')\n",
    "axs[0,1].axis('tight')\n",
    "axs[0,1].set_xlabel('alpha')\n",
    "axs[0,1].set_ylabel('weights')\n",
    "axs[0,1].legend(boston.feature_names)\n",
    "\n",
    "axs[1,0].plot(alphas, r2_ridge)\n",
    "axs[1,0].set_xscale('log')\n",
    "axs[1,0].set_title('Ridge scores as a function of the regularization')\n",
    "axs[1,0].axis('tight')\n",
    "axs[1,0].set_xlabel('alpha')\n",
    "axs[1,0].set_ylabel('r2')\n",
    "\n",
    "axs[1,1].plot(alphas, r2_lasso)\n",
    "axs[1,1].set_xscale('log')\n",
    "axs[1,1].set_title('Lasso scores as a function of the regularization')\n",
    "axs[1,1].axis('tight')\n",
    "axs[1,1].set_xlabel('alpha')\n",
    "axs[1,1].set_ylabel('r2')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal alphas\n",
    "best_r2_ridge = max(r2_ridge)\n",
    "max_index_ridge = r2_ridge.index(best_r2_ridge)\n",
    "best_alpha_ridge = alphas[max_index_ridge]\n",
    "print(max_index_ridge, best_alpha_ridge, best_r2_ridge, r2_ridge[max_index_ridge])\n",
    "\n",
    "best_r2_lasso = max(r2_lasso)\n",
    "max_index_lasso = r2_lasso.index(best_r2_lasso)\n",
    "best_alpha_lasso = alphas[max_index_lasso]\n",
    "print(max_index_lasso, best_alpha_lasso, best_r2_lasso, r2_lasso[max_index_lasso])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r2score_test, r2score_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = linear_model.Lasso(alpha=best_alpha_lasso)\n",
    "lasso.fit(X_train_norm, y_train)\n",
    "coefs = pd.Series(np.abs(lasso.coef_), features).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X_train_norm, columns=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['targ'] = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df[['RM', 'NOX', 'DIS', 'PTRATIO', 'targ']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selección de características con Sklearn\n",
    "\n",
    "También podemos seleccionar las características más importantes con sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.feature_selection as fs \n",
    "selector = fs.SelectKBest(score_func = fs.f_regression, k=5)\n",
    "\n",
    "X_new_train = selector.fit_transform(X_train,y_train)\n",
    "X_new_test = selector.transform(X_test)\n",
    "\n",
    "print('Non important variables: {}'.format(boston.feature_names[selector.get_support()==False]))\n",
    "print('Relevant variables: {}'.format(boston.feature_names[selector.get_support()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El conjunto de características seleccionadas ahora es diferente, ya que el criterio ha cambiado.\n",
    "\n",
    "El método SelectKBest selecciona características según las k puntuaciones más altas. La puntuación se calcula usando la función score_func.\n",
    "En este caso, utilizamos f_regression como nuestra función de puntuación, que devuelve la estadística F y los valores p de las pruebas de regresión lineal univariante de cada característica en X contra y.\n",
    "\n",
    "**EJERCICIO 3** Diabetes:\n",
    "\n",
    "El conjunto de datos de diabetes (de scikit-learn) consta de 10 variables fisiológicas (edad, sexo, peso, presión arterial) medidas en 442 pacientes, y una indicación de la progresión de la enfermedad después de un año.\n",
    "\n",
    "Exploraremos el rendimiento del modelo de Regresión Lineal y el modelo LASSO para la predicción.\n",
    "\n",
    "Completa los huecos del ejercicio.\n",
    "\n",
    "Carga los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "diabetes = datasets.load_diabetes()\n",
    "X,y = diabetes.data, diabetes.target\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = diabetes.feature_names\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evalúa la predicción usando un modelo de regresión simple y un modelo de regresión múltiple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X, columns=features)\n",
    "df['targ'] = y\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_mat = np.abs(df.corr())\n",
    "sns.heatmap(corr_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data=df[['bmi', 's5', 'bp', 's4', 'targ']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.2)\n",
    "print(df_train.shape, df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el modelo simple, primero elige una de las dimensiones de los datos. Intenta realizar algunos gráficos para identificar posibles relaciones lineales entre las variables predictoras y las variables objetivo. Elige una variable para tu primer modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train[['s5']]\n",
    "y_train = df_train['targ']\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df_test[['s5']]\n",
    "y_test = df_test['targ']\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LinearRegression()\n",
    "lm.fit(X_train, y_train)\n",
    "print(lm.score(X_train, y_train), lm.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide en conjuntos de entrenamiento y prueba y evalúa la predicción (sklearn) con un modelo de regresión múltiple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.drop(columns='targ')\n",
    "y_train = df_train['targ']\n",
    "\n",
    "X_test = df_test.drop(columns='targ')\n",
    "y_test = df_test['targ']\n",
    "\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LinearRegression()\n",
    "lm.fit(X_train, y_train)\n",
    "print(lm.score(X_train, y_train), lm.score(X_test, y_test))\n",
    "\n",
    "print(lm.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el modelo de regresión múltiple, divide en conjuntos de entrenamiento y prueba y evalúa la predicción (sklearn) sin y con regularización LASSO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_new = scaler.fit_transform(X_train)\n",
    "X_test_new = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = linear_model.Lasso(alpha=1)\n",
    "lm.fit(X_train_new, y_train)\n",
    "print(lm.score(X_train_new, y_train), lm.score(X_test_new, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(np.abs(lm.coef_), features).sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Almost the same results with less \"activated\" coefficients (the result has 3 zero coefficients).\n",
    " \n",
    " Is the score different? How many predictors are we using now?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EJERCICIO 4: Ventas de Big Mart**\n",
    "\n",
    "Usa el [conjunto de datos de ventas de Big Mart](https://www.kaggle.com/brijbhushannanda1979/bigmart-sales-data). En el conjunto de datos, tenemos ventas de productos por producto para múltiples sucursales de una cadena.\n",
    "\n",
    "En particular, podemos ver características del artículo vendido (contenido de grasa, visibilidad, tipo, precio) y algunas características de la sucursal (año de establecimiento, tamaño, ubicación, tipo) y el número de artículos vendidos para ese artículo en particular. Veamos si podemos predecir las ventas usando estas características.\n",
    "\n",
    "Implementa el siguiente análisis:\n",
    "- Lee los archivos de entrenamiento y prueba en un DataFrame de pandas\n",
    "- Limpia los datos (hay algunos valores faltantes)\n",
    "- Convierte las variables categóricas en valores numéricos y excluye 'Item_Identifier' y 'Item_Outlet_Sales' (que es el objetivo).\n",
    "- Estudia cuáles son las variables con mayor (menor) correlación con la variable objetivo.\n",
    "- Aplica regresión lineal usando todas las características.\n",
    "- Construye el gráfico de residuos y da una interpretación del mismo\n",
    "- Elige un modelo de regresión polinómica para ajustar mejor los datos.\n",
    "- Compara los regresores de ridge y lasso.\n",
    "- Compara la magnitud de los coeficientes de los diferentes modelos.\n",
    "- Estima cuáles son las mejores características para la predicción.\n",
    "\n",
    "Lee los archivos de entrenamiento y prueba en un DataFrame de pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data:\n",
    "df_train = pd.read_csv('files/ch06/bigmart-sales-data/Train.csv')\n",
    "\n",
    "df_test = pd.read_csv('files/ch06/bigmart-sales-data/test.csv')\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limpia los datos (hay algunos valores faltantes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observa los datos faltantes en Item_Weight y Outlet_Size. Veamos cómo se ven estas variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train.Item_Weight.mean(), df_train.Item_Weight.std())\n",
    "plt.hist(df_train.Item_Weight, bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace nulls Item_Weight in with mean\n",
    "mean_Item_Weight = df_train.Item_Weight.mean()\n",
    "df_train2 =  df_train.copy()\n",
    "df_test2 =  df_test.copy()\n",
    "\n",
    "print(mean_Item_Weight)\n",
    "df_train2[['Item_Weight']] = df_train2[['Item_Weight']].fillna(value=mean_Item_Weight)\n",
    "df_test2[['Item_Weight']] = df_test2[['Item_Weight']].fillna(value=mean_Item_Weight)\n",
    "df_train2[df_train.Item_Weight.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=\"Outlet_Size\", data=df_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train2[['Outlet_Size']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll fill the empty values of Outlet_Size with medium, since it´s the most common value.\n",
    "df_train2[['Outlet_Size']] = df_train2[['Outlet_Size']].fillna(value='Medium')\n",
    "df_test2[['Outlet_Size']] = df_test2[['Outlet_Size']].fillna(value='Medium')\n",
    "df_train2[['Outlet_Size']].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convierte las variables categóricas en valores numéricos y excluye 'Item_Identifier' y 'Item_Outlet_Sales' (que es el objetivo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_train2['Item_Outlet_Sales']\n",
    "\n",
    "df_train2.drop(columns=['Item_Identifier','Item_Outlet_Sales'], inplace=True)\n",
    "df_test2.drop(columns=['Item_Identifier'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_num = ['Item_Weight', 'Item_Visibility', 'Item_MRP','Outlet_Establishment_Year']\n",
    "cols_cat = ['Item_Fat_Content', 'Item_Type', 'Outlet_Identifier', 'Outlet_Size', 'Outlet_Location_Type', 'Outlet_Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cols_cat:\n",
    "    print()\n",
    "    print(df_train2[[col]].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train2 = pd.get_dummies(df_train2, drop_first=True)\n",
    "df_test2 = pd.get_dummies(df_test2, drop_first=True)\n",
    "df_test2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estudia cuáles son las variables con la mayor (menor) correlación con la variable objetivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_num = ['Item_Weight', 'Item_Visibility', 'Item_MRP', 'Outlet_Establishment_Year']\n",
    "X_num = df_train2[cols_num].values\n",
    "X_num_test = df_test2[cols_num].values\n",
    "(X_num.shape, X_num_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scalerX = StandardScaler().fit(X_num) # Create the transformer StandardScaler and perform the fitting for the training data\n",
    "\n",
    "X_num = scalerX.transform(X_num)\n",
    "X_num_test = scalerX.transform(X_num_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train2[cols_num] = X_num\n",
    "df_test2[cols_num] = X_num_test\n",
    "df_train2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train2['y'] = y\n",
    "corr_mat = np.abs(df_train2.corr())\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "sns.heatmap(corr_mat, square=True, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_mat.y.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplica regresión lineal utilizando todas las características."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train2.drop(columns=['y'])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X, y)\n",
    "\n",
    "y_pred = lr.predict(X)\n",
    "\n",
    "print(lr.score(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construye el gráfico de residuos y proporciona una interpretación del mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x = y_pred, y = y - y_pred, alpha=0.4)\n",
    "\n",
    "plt.hlines(y=0, xmin= 0, xmax=y_pred.max())\n",
    "plt.title('Residual plot')\n",
    "plt.xlabel('$\\hat y$')\n",
    "plt.ylabel('$y - \\hat y$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y, y_pred, alpha=0.3)\n",
    "\n",
    "plt.plot([y.min(), y.max()], [y.min(), y.max()], '--k')\n",
    "plt.axis('tight')\n",
    "plt.xlabel('$y$')\n",
    "plt.ylabel('$\\hat y$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elige un modelo de regresión polinómica para ajustar mejor los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import linear_model\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X2 = poly.fit_transform(X)\n",
    "\n",
    "clf = linear_model.LinearRegression()\n",
    "clf.fit(X2, y)\n",
    "\n",
    "print(clf.score(X2, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compara los regresores de ridge y lasso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "#lr = LinearRegression()\n",
    "#lr.fit(X, y)\n",
    "\n",
    "ridge = Ridge(alpha=.5)\n",
    "ridge.fit(X, y)\n",
    "\n",
    "lasso = Lasso(alpha=.5)\n",
    "lasso.fit(X, y)\n",
    "\n",
    "print(lr.score(X, y), ridge.score(X, y), lasso.score(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compara la magnitud de los coeficientes de los diferentes modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs_lr = pd.Series(lr.coef_, df_train2.columns[:-1]).sort_values()\n",
    "coefs_ridge = pd.Series(ridge.coef_, df_train2.columns[:-1]).sort_values()\n",
    "coefs_lasso = pd.Series(lasso.coef_, df_train2.columns[:-1]).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(coefs_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(coefs_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(coefs_lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estima cuáles son las mejores características para la predicción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_train2.columns[:-1]\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.feature_selection as fs \n",
    "selector = fs.SelectKBest(score_func = fs.f_regression,k=5)\n",
    "\n",
    "X_new = selector.fit_transform(X,y)\n",
    "\n",
    "print('Non important variables: {}'.format(features[selector.get_support()==False]))\n",
    "print('Relevant variables: {}'.format(features[selector.get_support()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANÁLISIS ADICIONAL PARA LOS DATOS DE BOSTON**\n",
    "\n",
    "Para comparar el ajuste de los modelos de regresión lineal y polinómica también podemos usar la biblioteca sklearn.\n",
    "\n",
    "A continuación, añadimos una evaluación cuantitativa de los dos modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = datasets.load_boston()\n",
    "X_boston,y_boston = boston.data, boston.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of the linear model\n",
    "X_boston,y_boston = boston.data, boston.target\n",
    "\n",
    "regr_boston = LinearRegression()\n",
    "regr_boston.fit(X_boston, y_boston) \n",
    "\n",
    "#print('Coeff and intercept: {} {}'.format(regr_boston.coef_, regr_boston.intercept_))\n",
    "print('Multiple Linear regression Score: {}'.format(regr_boston.score(X_boston, y_boston)))\n",
    "print('Multiple Linear regression MSE: {}'.format(np.mean((regr_boston.predict(X_boston) - y_boston)**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of the polynomial model\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "regr_pol = Pipeline([('poly', PolynomialFeatures(degree=2)),('linear', LinearRegression(fit_intercept=False))])\n",
    "regr_pol.fit(X_boston, y_boston) \n",
    "\n",
    "#print('Coeff and intercept: {} {}'.format(regr_pol.named_steps['linear'].coef_, regr_pol.named_steps['linear'].intercept_))\n",
    "print('Multiple Polynomial regression Score: {}'.format(regr_pol.score(X_boston, y_boston)))\n",
    "print('Multiple Polynomial regression MSE: {}'.format(np.mean((regr_pol.predict(X_boston) - y_boston)**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la regresión simple, primero necesitamos extraer una de las características y luego usar los mismos métodos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantitative evaluation of the SIMPLE lineal and polynomial regression:\n",
    "bostonDF = pd.DataFrame(boston.data)\n",
    "bostonDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bostonDF.columns=boston.feature_names \n",
    "bostonDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=bostonDF['LSTAT']\n",
    "y=boston.target\n",
    "x = np.expand_dims(x, axis=1)\n",
    "y = np.expand_dims(y, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr_boston = LinearRegression()\n",
    "regr_boston.fit(x, y) \n",
    "\n",
    "print('Simple linear regression Score: {}'.format(regr_boston.score(x, y)))\n",
    "print('Simple linear regression MSE: {}'.format(np.mean((regr_boston.predict(x) - y)**2)))\n",
    "\n",
    "regr_pol = Pipeline([('poly', PolynomialFeatures(degree=2)),('linear', LinearRegression(fit_intercept=False))])\n",
    "regr_pol.fit(x, y) \n",
    "\n",
    "print('Simple Polynomial regression (order 2) Score: {}'.format(regr_pol.score(x, y)))\n",
    "print('Simple Polynomial regression (order 2) MSE: {}'.format(np.mean((regr_pol.predict(x) - y)**2)))\n",
    "\n",
    "regr_pol = Pipeline([('poly', PolynomialFeatures(degree=3)),('linear', LinearRegression(fit_intercept=False))])\n",
    "regr_pol.fit(x, y) \n",
    "\n",
    "print('Simple Polynomial regression (order 3) Score: {}'.format(regr_pol.score(x, y)))\n",
    "print('Simple Polynomial regression (order 3) MSE: {}'.format(np.mean((regr_pol.predict(x) - y)**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EJERCICIO 2: Conjunto de datos macroeconómicos**\n",
    "\n",
    "Para comenzar, cargamos el conjunto de datos Longley de datos macroeconómicos de EE. UU. desde el sitio web de conjuntos de datos de R. Datos macroeconómicos desde 1947 hasta 1962."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "df = pd.read_csv('http://vincentarelbundock.github.io/Rdatasets/csv/datasets/longley.csv', index_col=0)\n",
    "df.head()\n",
    "\n",
    "# Clean column names\n",
    "df.columns = ['GNPdeflator', 'GNP', 'Unemployed', 'ArmedForces', 'Population','Year', 'Employed']\n",
    "features = ['GNPdeflator','Unemployed','ArmedForces','Population','Year','Employed']\n",
    "target = 'GNP'\n",
    "\n",
    "# Create X matrix and y vector from the dataset\n",
    "X = df[features].values\n",
    "y = df[target].values\n",
    "\n",
    "print('Shape of data: {} {}'.format(X.shape, y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting a multiple linear model\n",
    "lin_reg = LinearRegression() # Create the Linear Regression estimator\n",
    "lin_reg.fit(X, y) # Perform the fitting\n",
    "\n",
    "\n",
    "# Regrerssion coefs\n",
    "coefs = pd.Series(lin_reg.coef_, features).sort_values()\n",
    "\n",
    "# Prediction\n",
    "y_pred = lin_reg.predict(X)\n",
    "\n",
    "# evaluation\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "r2score = lin_reg.score(X, y)\n",
    "\n",
    "# The coefficients\n",
    "print('\\nIntercept and coefs:\\n{} {}'.format(lin_reg.intercept_, lin_reg.coef_))\n",
    "# The mean squared error\n",
    "print('\\nMSE: {}'.format(mse))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print('R^2 Score: {}'.format(r2score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting abs value of model coefficients\n",
    "np.abs(coefs).sort_values().plot(kind='bar', title='Model Coefficients')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
